import difflib
from collections import Counter, defaultdict
import re
import os

class HTRErrorAnalyzer:
    def __init__(self):
        self.error_stats = {
            'substitutions': Counter(),
            'insertions': Counter(), 
            'deletions': Counter()
        }
        self.word_errors = []
        
    def read_file_with_fallback_encoding(self, filepath):
        """Essaie plusieurs encodages pour lire un fichier"""
        encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
        
        for encoding in encodings:
            try:
                with open(filepath, 'r', encoding=encoding) as f:
                    content = f.read().strip()
                    print(f"‚úì Fichier '{os.path.basename(filepath)}' lu avec l'encodage {encoding}")
                    return content
            except UnicodeDecodeError:
                continue
        
        raise ValueError(f"Impossible de lire {filepath} avec les encodages test√©s: {encodings}")
        
    def analyze_texts(self, ground_truth, hypothesis, text_name=""):
        """Analyse les diff√©rences entre v√©rit√© de terrain et hypoth√®se HTR"""
        print(f"\n=== Analyse: {text_name} ===")
        print("=" * (15 + len(text_name)))
        
        # Analyse au niveau caract√®re
        self._analyze_characters(ground_truth, hypothesis)
        
        # Analyse au niveau mot
        self._analyze_words(ground_truth, hypothesis)
        
        # Visualisation HTML des diff√©rences
        html_diff = self._generate_html_diff(ground_truth, hypothesis, text_name)
        
        return html_diff
    
    def _analyze_characters(self, gt, hyp):
        """Analyse des erreurs au niveau caract√®re"""
        print("\nüìù ANALYSE CARACT√àRES")
        print("-" * 40)
        
        # Calcul de la distance de Levenshtein avec d√©tails
        matcher = difflib.SequenceMatcher(None, gt, hyp)
        
        char_errors = {'substitutions': 0, 'insertions': 0, 'deletions': 0}
        
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'replace':
                # Substitutions
                gt_chars = gt[i1:i2]
                hyp_chars = hyp[j1:j2]
                char_errors['substitutions'] += max(len(gt_chars), len(hyp_chars))
                
                # Enregistrer les substitutions fr√©quentes
                for g, h in zip(gt_chars, hyp_chars):
                    if g != h:
                        self.error_stats['substitutions'][f"'{g}' ‚Üí '{h}'"] += 1
                        
            elif tag == 'delete':
                # Suppressions 
                deleted = gt[i1:i2]
                char_errors['deletions'] += len(deleted)
                for char in deleted:
                    self.error_stats['deletions'][f"'{char}'"] += 1
                    
            elif tag == 'insert':
                # Insertions
                inserted = hyp[j1:j2]
                char_errors['insertions'] += len(inserted)
                for char in inserted:
                    self.error_stats['insertions'][f"'{char}'"] += 1
        
        # Calcul du CER (Character Error Rate)
        total_chars = len(gt)
        total_errors = sum(char_errors.values())
        cer = (total_errors / total_chars) * 100 if total_chars > 0 else 0
        
        print(f"Longueur v√©rit√© terrain: {len(gt)} caract√®res")
        print(f"Longueur hypoth√®se: {len(hyp)} caract√®res")
        print(f"CER (Character Error Rate): {cer:.2f}%")
        print(f"Substitutions: {char_errors['substitutions']}")
        print(f"Insertions: {char_errors['insertions']}")
        print(f"Suppressions: {char_errors['deletions']}")
    
    def _analyze_words(self, gt, hyp):
        """Analyse des erreurs au niveau mot"""
        print("\nüî§ ANALYSE MOTS")
        print("-" * 40)
        
        # Tokenisation simple (peut √™tre am√©lior√©e selon vos besoins)
        gt_words = re.findall(r'\b\w+\b', gt.lower())
        hyp_words = re.findall(r'\b\w+\b', hyp.lower())
        
        # Alignement des mots
        matcher = difflib.SequenceMatcher(None, gt_words, hyp_words)
        
        word_errors = {'correct': 0, 'substitutions': 0, 'insertions': 0, 'deletions': 0}
        
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal':
                word_errors['correct'] += (i2 - i1)
            elif tag == 'replace':
                word_errors['substitutions'] += max(i2-i1, j2-j1)
                # Enregistrer les erreurs de mots
                for gt_word, hyp_word in zip(gt_words[i1:i2], hyp_words[j1:j2]):
                    if gt_word != hyp_word:
                        self.word_errors.append({
                            'ground_truth': gt_word,
                            'hypothesis': hyp_word,
                            'error_type': 'substitution'
                        })
            elif tag == 'delete':
                word_errors['deletions'] += (i2 - i1)
                for word in gt_words[i1:i2]:
                    self.word_errors.append({
                        'ground_truth': word,
                        'hypothesis': '',
                        'error_type': 'deletion'
                    })
            elif tag == 'insert':
                word_errors['insertions'] += (j2 - j1)
                for word in hyp_words[j1:j2]:
                    self.word_errors.append({
                        'ground_truth': '',
                        'hypothesis': word,
                        'error_type': 'insertion'
                    })
        
        # Calcul du WER (Word Error Rate)
        total_words = len(gt_words)
        total_word_errors = word_errors['substitutions'] + word_errors['insertions'] + word_errors['deletions']
        wer = (total_word_errors / total_words) * 100 if total_words > 0 else 0
        
        print(f"Nombre de mots (v√©rit√© terrain): {len(gt_words)}")
        print(f"Nombre de mots (hypoth√®se): {len(hyp_words)}")
        print(f"WER (Word Error Rate): {wer:.2f}%")
        print(f"Mots corrects: {word_errors['correct']}")
        print(f"Substitutions de mots: {word_errors['substitutions']}")
        print(f"Insertions de mots: {word_errors['insertions']}")
        print(f"Suppressions de mots: {word_errors['deletions']}")
    
    def _generate_html_diff(self, gt, hyp, model_name=""):
        """G√©n√®re une visualisation HTML des diff√©rences"""
        differ = difflib.HtmlDiff(tabsize=2)
        html_diff = differ.make_file(
            gt.splitlines(keepends=True),
            hyp.splitlines(keepends=True),
            fromdesc='V√©rit√© de terrain',
            todesc=f'Hypoth√®se HTR - {model_name}' if model_name else 'Hypoth√®se HTR',
            context=True,
            numlines=3
        )
        return html_diff
    
    def print_error_summary(self, top_n=10):
        """Affiche un r√©sum√© des erreurs les plus fr√©quentes"""
        print(f"\nüîç R√âSUM√â DES ERREURS LES PLUS FR√âQUENTES")
        print("=" * 50)
        
        if self.error_stats['substitutions']:
            print(f"\nüìä Top {top_n} substitutions de caract√®res:")
            for error, count in self.error_stats['substitutions'].most_common(top_n):
                print(f"  {error}: {count} fois")
        
        if self.error_stats['insertions']:
            print(f"\n‚ûï Top {top_n} insertions de caract√®res:")
            for error, count in self.error_stats['insertions'].most_common(top_n):
                print(f"  {error}: {count} fois")
        
        if self.error_stats['deletions']:
            print(f"\n‚ûñ Top {top_n} suppressions de caract√®res:")
            for error, count in self.error_stats['deletions'].most_common(top_n):
                print(f"  {error}: {count} fois")
        
        if self.word_errors:
            print(f"\nüî§ Erreurs de mots par type:")
            error_by_type = defaultdict(list)
            for error in self.word_errors:
                error_by_type[error['error_type']].append(error)
            
            for error_type, errors in error_by_type.items():
                print(f"\n  {error_type.upper()} ({len(errors)} erreurs):")
                for error in errors[:top_n]:
                    if error_type == 'substitution':
                        print(f"    '{error['ground_truth']}' ‚Üí '{error['hypothesis']}'")
                    elif error_type == 'deletion':
                        print(f"    '{error['ground_truth']}' ‚Üí (supprim√©)")
                    elif error_type == 'insertion':
                        print(f"    (rien) ‚Üí '{error['hypothesis']}'")
    
    def reset_stats(self):
        """Remet √† z√©ro les statistiques pour une nouvelle analyse"""
        self.error_stats = {
            'substitutions': Counter(),
            'insertions': Counter(), 
            'deletions': Counter()
        }
        self.word_errors = []

def main():
    analyzer = HTRErrorAnalyzer()
    
    # Chemins vers vos fichiers
    gt_file = "./DATA/ground_truth/R52_1_10p_GT.txt"
    hyp1_file = "./DATA/hypothesis/FT/R52_1_10p_FT.txt"
    hyp2_file = "/DATA/hypothesis/FT/R52_1_10p_FT_postcor.txt"
    
    print("üîç ANALYSE COMPARATIVE DES SORTIES HTR")
    print("=" * 60)
    
    try:
        # Lecture des fichiers
        print("\nüìÅ Lecture des fichiers...")
        ground_truth = analyzer.read_file_with_fallback_encoding(gt_file)
        hypothesis1 = analyzer.read_file_with_fallback_encoding(hyp1_file)
        hypothesis2 = analyzer.read_file_with_fallback_encoding(hyp2_file)
        
        print(f"‚úì Tous les fichiers ont √©t√© lus avec succ√®s")
        
        # Analyse de la premi√®re hypoth√®se
        html_diff1 = analyzer.analyze_texts(ground_truth, hypothesis1, "FT")
        
        # Analyse de la deuxi√®me hypoth√®se (on garde les stats cumul√©es)
        html_diff2 = analyzer.analyze_texts(ground_truth, hypothesis2, "FT_postcor")
        
        # R√©sum√© des erreurs cumul√©es
        analyzer.print_error_summary()
        
        # Sauvegarde des visualisations HTML
        print(f"\nüíæ Sauvegarde des visualisations...")
        
        with open('diff_visualization_FT.html', 'w', encoding='utf-8') as f:
            f.write(html_diff1)
            
        with open('diff_visualization_FT_postcor.html', 'w', encoding='utf-8') as f:
            f.write(html_diff2)
            
        print("‚úì Visualisations sauvegard√©es:")
        print("  - diff_visualization_FT.html")
        print("  - diff_visualization_FT_postcor.html")
        print("\nOuvrez ces fichiers dans votre navigateur pour voir les diff√©rences en couleur!")
        
    except FileNotFoundError as e:
        print(f"‚ùå Fichier non trouv√©: {e}")
        print("V√©rifiez que les chemins des fichiers sont corrects.")
        
    except ValueError as e:
        print(f"‚ùå Erreur d'encodage: {e}")
        
    except Exception as e:
        print(f"‚ùå Erreur inattendue: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()